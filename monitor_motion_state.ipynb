{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Constant Monitoring Method for Motion State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import fetch_data\n",
    "import numpy as np\n",
    "import correct_drift\n",
    "import datetime\n",
    "import boto3\n",
    "import json\n",
    "from io import BytesIO\n",
    "import importlib\n",
    "importlib.reload(correct_drift)\n",
    "\n",
    "CANSERVER_PARSED_BUCKET = 'matt3r-canserver-us-west-2'\n",
    "CANSERVER_EVENT_BUCKET = 'matt3r-canserver-event-us-west-2'\n",
    "IMU_BUCKET = 'matt3r-imu-us-west-2'\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "importlib.reload(fetch_data)\n",
    "\n",
    "# define constants\n",
    "STATIONARY_SPEED = 0.5\n",
    "SPEED_NOISE_WINDOW = 0.5\n",
    "BUFFER_TIME = 30\n",
    "\n",
    "# imput k3y data\n",
    "start_date_str = '2023-07-10'\n",
    "end_date_str = '2023-07-10'\n",
    "organization_id = 'hamid'\n",
    "k3y_id = '17700cf8'\n",
    "\n",
    "start_date = datetime.datetime.strptime(start_date_str, '%Y-%m-%d')\n",
    "end_date = datetime.datetime.strptime(end_date_str, '%Y-%m-%d')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Collect Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_events(k3y_id, org_id, start_date, end_date):\n",
    "    # get a list of all json files in the prefix and filter them to within the date range\n",
    "    response = s3_client.list_objects(Bucket=CANSERVER_EVENT_BUCKET, Prefix=org_id + '/' + 'k3y-' + k3y_id + '/')\n",
    "    all_keys = [item['Key'] for item in response['Contents']]\n",
    "    keys = [file for file in all_keys if file.split('.')[-1] == 'json'\n",
    "            and len(file.split('/')[-1]) == 15\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('.')[0], '%Y-%m-%d') >= start_date\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('.')[0], '%Y-%m-%d') <= end_date]\n",
    "    keys = sorted(keys, key=lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # retrieve and combine filtered json files\n",
    "    event_dict = {}\n",
    "    for key in keys:\n",
    "        response = s3_client.get_object(Bucket=CANSERVER_EVENT_BUCKET, Key=key)\n",
    "        result = json.loads(response[\"Body\"].read().decode())\n",
    "        for index in result['imu_telematics']:\n",
    "            if index in event_dict:\n",
    "                event_dict[index].extend(result['imu_telematics'][index])\n",
    "            else:\n",
    "                event_dict[index] = result['imu_telematics'][index]\n",
    "\n",
    "    return event_dict\n",
    "\n",
    "def get_can_data(k3y_id, org_id, start_date, end_date):\n",
    "    # get a list of all parquet files in the prefix and filter them to within the date range\n",
    "    response = s3_client.list_objects_v2(Bucket=CANSERVER_PARSED_BUCKET, Prefix=org_id + '/' + 'k3y-' + k3y_id + '/')\n",
    "    all_keys = [item['Key'] for item in response.get('Contents', [])]\n",
    "\n",
    "    while response['IsTruncated']:\n",
    "        response = s3_client.list_objects_v2(Bucket=CANSERVER_PARSED_BUCKET, Prefix=org_id + '/' + 'k3y-' + k3y_id + '/', ContinuationToken=response['NextContinuationToken'])\n",
    "        all_keys.extend([item['Key'] for item in response.get('Contents', [])])\n",
    "\n",
    "    keys = [file for file in all_keys if file.split('.')[-1] == 'parquet'\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('_')[0], '%Y-%m-%d') >= start_date\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('_')[0], '%Y-%m-%d') <= end_date]\n",
    "    keys = sorted(keys, key=lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # retrieve and combine filtered perquet files\n",
    "    df_list = []\n",
    "    for key in keys:\n",
    "        response = s3_client.get_object(Bucket=CANSERVER_PARSED_BUCKET, Key=key)\n",
    "        buffer = BytesIO(response['Body'].read())\n",
    "        can_df = pd.read_parquet(buffer, engine='pyarrow')\n",
    "        df_list.append(can_df)\n",
    "    can_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return can_df\n",
    "\n",
    "def fetch_imu_data(imu_k3y_id, organization_id, start_date, end_date):\n",
    "    # get a list of all parquet files in the prefix and filter them to within the date range\n",
    "    response = s3_client.list_objects(Bucket=IMU_BUCKET, Prefix=organization_id + '/' + 'k3y-' + imu_k3y_id + '/accel/')\n",
    "    all_keys = [item['Key'] for item in response['Contents']]\n",
    "    keys = [file for file in all_keys if file.split('.')[-1] == 'parquet'\n",
    "            and len(file.split('/')[-1].split('_')[0]) == 10\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('_')[0], '%Y-%m-%d') >= start_date\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('_')[0], '%Y-%m-%d') <= end_date]\n",
    "    keys = sorted(keys, key=lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # retrieve and combine filtered perquet files\n",
    "    df_list = []\n",
    "    for key in keys:\n",
    "        response = s3_client.get_object(Bucket=IMU_BUCKET, Key=key)\n",
    "        buffer = BytesIO(response['Body'].read())\n",
    "        imu_df = pd.read_parquet(buffer, engine='pyarrow')\n",
    "        df_list.append(imu_df)\n",
    "    imu_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "    return imu_df\n",
    "\n",
    "def fetch_time_data(imu_k3y_id, organization_id, start_date, end_date):\n",
    "    # create a 1 day buffer to capture any data on the boundaries\n",
    "    start_date = start_date - datetime.timedelta(days=1)\n",
    "    end_date = end_date + datetime.timedelta(days=1)\n",
    "    # get a list of all parquet files in the prefix and filter them to within the date range\n",
    "    response = s3_client.list_objects(Bucket=IMU_BUCKET, Prefix=organization_id + '/' + 'k3y-' + imu_k3y_id + '/infer/')\n",
    "    all_keys = [item['Key'] for item in response['Contents']]\n",
    "    keys = [file for file in all_keys if file.split('.')[-1] == 'parquet'\n",
    "            and len(file.split('/')[-1].split('.')[0]) != 10\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('.')[0].split('_')[-1], '%Y-%m-%d') >= start_date\n",
    "            and datetime.datetime.strptime(file.split('/')[-1].split('.')[0].split('_')[-1], '%Y-%m-%d') <= end_date]\n",
    "    keys = sorted(keys, key=lambda x: x.split('/')[-1].split('.')[0])\n",
    "\n",
    "    # retrieve and combine filtered perquet files\n",
    "    df_list = []\n",
    "    for key in keys:\n",
    "        response = s3_client.get_object(Bucket=IMU_BUCKET, Key=key)\n",
    "        buffer = BytesIO(response['Body'].read())\n",
    "        time_df = pd.read_parquet(buffer, engine='pyarrow')\n",
    "        df_list.append(time_df)\n",
    "    time_df = pd.concat(df_list, axis=0, ignore_index=True)\n",
    "\n",
    "    # drop any nan values\n",
    "    time_df.dropna(subset=['diff_sw_sys(second)', 'imu_sw_clock(epoch)', 'system_clock(epoch)'], inplace=True)\n",
    "    time_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return time_df\n",
    "\n",
    "def shift_time(imu_df, time_df):\n",
    "    # identify any jumps in the data\n",
    "    jump_limit = 2\n",
    "    jump_indexes = time_df[abs(time_df['diff_sw_sys(second)'].diff()) > jump_limit].index\n",
    "    jump_indexes = jump_indexes.append(pd.Index([time_df.index[-1]]))\n",
    "\n",
    "    # create a list of the slope segments\n",
    "    segments = []\n",
    "    index_start = 0\n",
    "    for index in jump_indexes:\n",
    "        seg_data = {}\n",
    "        seg_data['start_timestamp'] = time_df['imu_sw_clock(epoch)'].iloc[index_start]\n",
    "        seg_data['end_timestamp'] = time_df['imu_sw_clock(epoch)'].iloc[index]\n",
    "        seg_data['slope'], seg_data['intercept'] = np.polyfit(time_df['system_clock(epoch)'][index_start:index], \n",
    "                                                            time_df['diff_sw_sys(second)'][index_start:index], 1)\n",
    "        seg_data['offset'] = seg_data['slope'] * seg_data['start_timestamp'] + seg_data['intercept']\n",
    "        segments.append(seg_data)\n",
    "        index_start = index\n",
    "\n",
    "    for seg in segments:\n",
    "        imu_df_seg = imu_df[(imu_df['timestamp(epoch in sec)'] >= seg['start_timestamp'])\n",
    "                            & (imu_df['timestamp(epoch in sec)'] < seg['end_timestamp'])]\n",
    "        imu_df.loc[imu_df_seg.index, 'correct_timestamp'] = imu_df_seg['timestamp(epoch in sec)'].apply(\n",
    "            lambda x: x - (x - seg['start_timestamp']) * seg['slope'] - seg['offset'])\n",
    "\n",
    "    # drop any nan values\n",
    "    imu_df.dropna(inplace=True)\n",
    "    imu_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    return imu_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "imu_df = fetch_imu_data(k3y_id, organization_id, start_date, end_date)\n",
    "time_df = fetch_time_data(k3y_id, organization_id, start_date, end_date)\n",
    "event_dict = get_events(k3y_id, organization_id, start_date, end_date)\n",
    "can_df = get_can_data(k3y_id, organization_id, start_date, end_date)\n",
    "# correct the imu time\n",
    "imu_df = shift_time(imu_df, time_df)\n",
    "imu_df['norm_acc'] = np.sqrt(imu_df['lr_acc(m/s^2)']**2 + imu_df['bf_acc(m/s^2)']**2 + imu_df['vert_acc(m/s^2)']**2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter the Driving State Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_can_driving_data(can_df, imu_df):\n",
    "    # get the true driving state data\n",
    "    speed_df = can_df[can_df['speed'].notna()].copy()\n",
    "    speed_df.reset_index(drop=True, inplace=True)\n",
    "    speed_df['driving'] = abs(speed_df['speed']) > STATIONARY_SPEED\n",
    "    dr_start_times = speed_df[speed_df['driving'].astype(int).diff() == 1]['timestamp'].to_list()\n",
    "    dr_end_times = speed_df[speed_df['driving'].astype(int).diff() == -1]['timestamp'].to_list()\n",
    "\n",
    "    # get the driving states\n",
    "    dr_df_states = []\n",
    "    for i in range(min(len(dr_start_times),len(dr_end_times))):\n",
    "        # filter out noise\n",
    "        if dr_end_times[i] - dr_start_times[i] > SPEED_NOISE_WINDOW:\n",
    "            dr_df_states.append(imu_df[(imu_df['correct_timestamp'] >= dr_start_times[i]) \n",
    "                            & (imu_df['correct_timestamp'] <= dr_end_times[i])])\n",
    "    if not dr_df_states:\n",
    "        return pd.DataFrame(columns=imu_df.columns)\n",
    "    return pd.concat(dr_df_states, ignore_index=True)\n",
    "\n",
    "def get_imu_driving_data(imu_df, time_df):\n",
    "\n",
    "    time_df['motion_bin'] = time_df['motion_state'].apply(lambda x: x != 'stationary').astype(int)\n",
    "    dr_start_times = time_df[time_df['motion_bin'].diff() == 1]['system_clock(epoch)'].to_list()\n",
    "    dr_end_times = time_df[time_df['motion_bin'].diff() == -1]['system_clock(epoch)'].to_list()\n",
    "\n",
    "    imu_df['driving_state'] = imu_df['correct_timestamp'].apply(\n",
    "        lambda x: any(dr_start - BUFFER_TIME <= x <= dr_end for dr_start, dr_end in zip(dr_start_times, dr_end_times))\n",
    "    )\n",
    "\n",
    "    imu_dr_df = imu_df[imu_df['driving_state']]\n",
    "    return imu_dr_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "can_dr_df = get_can_driving_data(can_df, imu_df)\n",
    "imu_dr_df = get_imu_driving_data(imu_df, time_df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute the Validation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def TPR(can_dr_df, imu_dr_df):\n",
    "    dr_start_times = [state['start'] for state in event_dict['driving_state']]\n",
    "    dr_end_times = [state['end'] for state in event_dict['driving_state']]\n",
    "\n",
    "    proxy_set = set(imu_dr_df[imu_dr_df['correct_timestamp'].apply(lambda x: any(start <= x <= end for start, end in zip(dr_start_times, dr_end_times)))]['correct_timestamp'].to_list())\n",
    "    truth_set = set(can_dr_df['correct_timestamp'].to_list())\n",
    "\n",
    "    try:\n",
    "        tpr = len(truth_set.intersection(proxy_set)) / len(truth_set)\n",
    "    except ZeroDivisionError:\n",
    "        tpr = 0\n",
    "\n",
    "    return tpr\n",
    "\n",
    "TPR(can_dr_df, imu_dr_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def FPR(imu_df, imu_dr_df):\n",
    "    pk_start_times = [state['timestamp'][0] for state in event_dict['parked_state']]\n",
    "    pk_end_times = [state['timestamp'][1] for state in event_dict['parked_state']]\n",
    "\n",
    "    proxy_set = set(imu_dr_df[imu_dr_df['correct_timestamp'].apply(lambda x: any(start <= x <= end for start, end in zip(pk_start_times, pk_end_times)))]['correct_timestamp'].to_list())\n",
    "    truth_set = set(imu_df[imu_df['correct_timestamp'].apply(lambda x: any(start <= x <= end for start, end in zip(pk_start_times, pk_end_times)))]['correct_timestamp'].to_list())\n",
    "\n",
    "    try:\n",
    "        fpr = len(proxy_set) / len(truth_set)\n",
    "    except ZeroDivisionError:\n",
    "        fpr = 1\n",
    "    \n",
    "    return fpr\n",
    "\n",
    "FPR(imu_df, imu_dr_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
